{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical methods of deep learning: Homework assignment 4\n",
    "Submit solution by uploading to canvas, **by Friday, November 30th, 12:00**\n",
    "\n",
    "**The task.** Perform an experimental study of convergence of gradient descent for a basic model, and give some theoretical interpretation to the results.\n",
    "* Consider random training sets consisting of $N=20$ points $(\\mathbf x_n, y_n),$ where $\\mathbf x_n\\in \\mathbb R^d, y_n\\in \\mathbb R.$ Generate each $\\mathbf x_n$ and $y_n$ independently, using standard normal distribution. Consider fitting this training data by a network having at least two hidden layers and using the standard quadratic loss.\n",
    "* For $d=15$, choose a network architecture (sizes of the layers, the activation functions,..) and training parameters (weight initialization, learning rate, number of GD steps,..) so that the network reliably learns the training data (say with the final loss below $10^{-8}$ for 80% of random training sets). Provide a motivation for your choice and compare it to other choices. \n",
    "* What happens with training if the input dimension $d$ is significantly decreased (say to $d=5$ or $d=2$)? Does performance improve or deteriorate, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import *\n",
    "from torch.optim import SGD\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19764356,  1.67595248, -0.50506144,  0.66762533, -0.91967509,\n",
       "       -0.09560613, -0.33479748,  0.30792044,  0.42676831, -1.30765899,\n",
       "        1.06058597,  1.31657895, -0.66373204,  2.02305011, -0.71936897,\n",
       "        0.67108124,  0.6541714 , -0.6180394 , -0.8654515 ,  0.24384127])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = np.random.normal(size=(N, d))\n",
    "Y = np.random.normal(size=(N))\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(Net, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        inp = input_dim\n",
    "        for i in range(len(hidden_dims)):\n",
    "            layers.append(nn.Linear(inp, hidden_dims[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            inp = hidden_dims[i]\n",
    "            \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(inp, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.out(self.net(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0984269049885143e-14\n",
      "1.455016594947671e-14\n",
      "1.1744771652346785e-14\n",
      "1.3789664194048014e-14\n",
      "7.40102406850086e-15\n",
      "2.070565940925917e-14\n",
      "2.4183432695084293e-14\n",
      "2.120525977034049e-14\n",
      "1.3751673072297764e-14\n",
      "1.8209912405558158e-14\n",
      "\n",
      "Maximal loss for 10 iterations: 1.8209912405558158e-14\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "d = 15\n",
    "\n",
    "hidden_dims = [15, 15]\n",
    "\n",
    "n_epoches = 500\n",
    "n_iterations = 20\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    X = np.random.normal(size=(N, d))\n",
    "    Y = np.random.normal(size=(N))\n",
    "    net = Net(input_dim=d, hidden_dims=hidden_dims)\n",
    "    \n",
    "    \n",
    "    optimizer = SGD(net.parameters(), lr=0.01)\n",
    "    lf = nn.MSELoss()\n",
    "    \n",
    "    final_losses = []\n",
    "    for ep in range(n_epoches):\n",
    "        \n",
    "        for k in range(N):\n",
    "            out = net(Variable(torch.FloatTensor(X[k])))\n",
    "            optimizer.zero_grad()\n",
    "            loss = lf(out, torch.FloatTensor([Y[k]]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    out = net(Variable(torch.FloatTensor(X)))        \n",
    "    final_loss = lf(out, torch.FloatTensor([Y]).t())\n",
    "    final_losses.append(final_loss.item())\n",
    "    \n",
    "    print(final_loss.item())\n",
    "\n",
    "print(f\"\\nMaximal loss for {n_iterations} iterations: {max(final_losses)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
