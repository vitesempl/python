{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical methods of deep learning: Homework assignment 1\n",
    "Submit solution by uploading to canvas, **by Friday, November 9th, 12:00**\n",
    "\n",
    "**The task.** Design a ReLU network with fewer than 10000 connections that approximates the function $f(x)=\\sin x$ on the segment $[-\\pi, \\pi]$ with uniform error not greater than $10^{-12}$. Implement the network in a Python notebook. Count the number of connections and demonstrate that the error bound is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \"\"\"\n",
    "    def __init__ (self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
    "        #dummy layer does nothing\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes an input data of shape [batch,input_units], returns output data [batch,output_units]\n",
    "        \"\"\"\n",
    "        #The dummy layer just returns whatever it gets as input.\n",
    "        return input\n",
    "\n",
    "    def backward(self,input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
    "        \"\"\"\n",
    "        #The gradient of dummy layer is precisely grad_output, but we'll write it more explicitly\n",
    "        num_units = input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        \n",
    "        return np.dot(grad_output,d_layer_d_input) #chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        pass\n",
    "    def forward(self,input):\n",
    "        \"\"\"apply elementwise ReLU to [batch,input_units] matrix\"\"\"\n",
    "        return np.maximum(0,input)\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        \"\"\"compute gradient of loss w.r.t. ReLU input\"\"\"\n",
    "        \n",
    "        relu_grad = input>0#<elementwise gradient of sigmoid output w.r.t. sigmoid input>\n",
    "        \n",
    "        #This time we use elemwise product instead of dot cuz sigmoid_grad is written elementwise\n",
    "        return grad_output*relu_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some tests\n",
    "\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "\n",
    "l = ReLU()\n",
    "\n",
    "grads = l.backward(x,np.ones([10,32])/(32*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.      ]\n",
      " [0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.      ]\n",
      " [0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.      ]\n",
      " [0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.      ]\n",
      " [0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  0.       0.       0.       0.       0.       0.       0.       0.      ]\n",
      " [0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125]\n",
      " [0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125]\n",
      " [0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125]\n",
      " [0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125]\n",
      " [0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
      "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125]]\n"
     ]
    }
   ],
   "source": [
    "print (grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
