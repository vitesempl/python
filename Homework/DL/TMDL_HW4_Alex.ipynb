{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical methods of deep learning: Homework assignment 4\n",
    "Submit solution by uploading to canvas, **by Friday, November 30th, 12:00**\n",
    "\n",
    "**The task.** Perform an experimental study of convergence of gradient descent for a basic model, and give some theoretical interpretation to the results.\n",
    "* Consider random training sets consisting of $N=20$ points $(\\mathbf x_n, y_n),$ where $\\mathbf x_n\\in \\mathbb R^d, y_n\\in \\mathbb R.$ Generate each $\\mathbf x_n$ and $y_n$ independently, using standard normal distribution. Consider fitting this training data by a network having at least two hidden layers and using the standard quadratic loss.\n",
    "* For $d=15$, choose a network architecture (sizes of the layers, the activation functions,..) and training parameters (weight initialization, learning rate, number of GD steps,..) so that the network reliably learns the training data (say with the final loss below $10^{-8}$ for 80% of random training sets). Provide a motivation for your choice and compare it to other choices. \n",
    "* What happens with training if the input dimension $d$ is significantly decreased (say to $d=5$ or $d=2$)? Does performance improve or deteriorate, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation for architecture:\n",
    "- Linear+ReLU layers are a pretty flexible choice (also for overfitting on training dataset) and the most popular choice of layer functions across recent years\n",
    "- Sizes of the layers should not be below input size or output size to avoid introducing a bottleneck (because full memorization is the intent). Hidden size = input size often works well in practice (the theorem on deep narrow networks doesn't work here since depth is limited).\n",
    "- Learning rate choice is standard and was empirically shown to achieve good performance on an extremely wide range of tasks.\n",
    "- Number of GD steps should be set at maximum until the desired accuracy on training set in achieved (in this specific case of overfitting). It is just limited by running time which should definitely not exceed time left until homework deadline.\n",
    "- Weight initialization is uniform on an interval dependend on layer size. It was introduced to avoid vanishing/exploding gradients with saturating activation functions. With ReLU it doesn't matter much but this is still a common heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import *\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.optim import SGD\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(len(hidden_dims)):\n",
    "            prev_dim = input_dim if i == 0 else hidden_dims[i-1]\n",
    "            cur_layer = nn.Linear(prev_dim, hidden_dims[i])\n",
    "#             init.normal(self.lin.weight, std=0.0001)\n",
    "#             init.normal(self.lin.bias, std=0.0001)\n",
    "            self.layers.append(cur_layer)\n",
    "            self.layers.append(nn.ReLU())\n",
    "            \n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "        self.out = nn.Linear(hidden_dims[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.out(self.net(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c880fc296f41c39baa59388c68100e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.56798476\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab225b121ac04aa0a3c573521181a14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.163595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc915d3e3b44708a7a56441d54b5d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.36478776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7cf56ae0c441e3b8d3e91147ee5c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.53142613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8a05abda154e9d8261e2471dde2bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.20229197\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40353d9355be42ecaf7d51a977a27e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.6466337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7a3b03cf9a486784cd9c241b492505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.16524045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462a00ec4e7f4769859a8d6b67f031cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.3159274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9939b8706f934a3485a230e7d9436ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.35452053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017761723405479d9d40441f91568c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.4477297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08a26b15bca406abb0a90d5d809c54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.21858501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372eafcd71374a5d8d4b04ca2cea7cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.86649925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b488bfee9f4b48cf91e42810e9343302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.06990448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe89aa00fbc746468565d554004f1523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.18400669\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4158371a7ac4a7e955164147c5e2c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.34196514\n"
     ]
    }
   ],
   "source": [
    "n_iters = 100\n",
    "\n",
    "N = 20\n",
    "\n",
    "d = 2\n",
    "# for d in range(2, 15):\n",
    "# for i in range(15):\n",
    "hidden_dim = 45\n",
    "hidden_dims = [hidden_dim] * 2\n",
    "# new one\n",
    "\n",
    "outcomes = []\n",
    "n_epochs = 15\n",
    "for i in range(n_epochs):\n",
    "    X = np.random.normal(size=(N, d))\n",
    "    Y = np.random.normal(size=(N))\n",
    "    net = Net(input_dim=d, hidden_dims=hidden_dims)\n",
    "\n",
    "    optimizer = SGD(net.parameters(), lr=0.01)\n",
    "    lf = nn.MSELoss()\n",
    "\n",
    "    x_losses = []\n",
    "    for ep in tnrange(n_iters):\n",
    "        for k in range(N):\n",
    "            input_ = Variable(torch.FloatTensor(X[k])).view(1, d)\n",
    "            out = net(input_)\n",
    "            optimizer.zero_grad()\n",
    "            target = Variable(torch.FloatTensor([Y[k]]).view(1, 1))\n",
    "            loss = lf(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        out = net(Variable(torch.FloatTensor(X)))\n",
    "        final_loss = lf(out, Variable(torch.FloatTensor([Y]).t()))\n",
    "        x_losses.append(final_loss.data.numpy())\n",
    "\n",
    "    out = net(Variable(torch.FloatTensor(X)))\n",
    "    final_loss = lf(out, Variable(torch.FloatTensor([Y]).t()))\n",
    "\n",
    "    outcome_val = final_loss.data.numpy()\n",
    "    print(outcome_val)\n",
    "#     plt.semilogy(x_losses)\n",
    "#     plt.suptitle(f'{d}')\n",
    "#     plt.show()\n",
    "\n",
    "    outcomes.append(outcome_val < 10e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.8988382, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
